# Core Utils â€” Complete Implementation Plan

All requirements: `EnvironmentConfig` class, `setup.py` with entry points, YAML config loader, `read_source` dispatcher with format readers, Delta writer, DAB job configs, and end-to-end wiring.

---

## 1. Package Structure

```
core_utils/
â”œâ”€â”€ setup.py
â”œâ”€â”€ requirements.txt                       # pyyaml (pyspark provided by Databricks)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ core_utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ entrypoints.py                 # CLI entry points: run-bronze / run-silver / run-gold
â”‚       â”œâ”€â”€ core/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ environment_config.py      # â˜… WorkspaceID â†’ env â†’ catalog/schema/paths
â”‚       â”œâ”€â”€ bronze/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ config_loader.py           # Load + validate YAML config
â”‚       â”‚   â”œâ”€â”€ readers.py                 # read_source dispatcher + read_csv/parquet/txt/json/binary
â”‚       â”‚   â””â”€â”€ writer.py                  # write_to_target (append / overwrite / merge)
â”‚       â”œâ”€â”€ silver/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ silver_runner.py           # (future)
â”‚       â””â”€â”€ gold/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ gold_runner.py             # (future)
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ bronze/
â”‚       â”œâ”€â”€ sample_csv.yml
â”‚       â”œâ”€â”€ sample_parquet.yml
â”‚       â””â”€â”€ sample_txt.yml
â””â”€â”€ tests/
    â”œâ”€â”€ test_environment_config.py
    â”œâ”€â”€ test_config_loader.py
    â””â”€â”€ test_readers.py
```

---

## 2. `setup.py`

```python
from setuptools import setup, find_packages

setup(
    name="core_utils",
    version="1.0.0",
    description="Bronze/Silver/Gold ETL framework for Databricks",
    author="Your Team",
    package_dir={"": "src"},
    packages=find_packages(where="src"),
    python_requires=">=3.9",
    install_requires=[
        "pyyaml>=6.0",
        # pyspark NOT listed â€” provided by Databricks runtime
    ],
    entry_points={
        "console_scripts": [
            "run-bronze=core_utils.entrypoints:bronze_entry",
            "run-silver=core_utils.entrypoints:silver_entry",
            "run-gold=core_utils.entrypoints:gold_entry",
        ],
    },
)
```

> [!IMPORTANT]
> `pyspark` is intentionally **NOT** in `install_requires`. Databricks provides it. Including it causes conflicts.

---

## 3. `environment_config.py` â€” EnvironmentConfig Class

Detects workspace â†’ resolves environment â†’ builds all paths dynamically.

**Two modes:**
- **Initialized with `catalog`/`schema`** â†’ methods return those defaults
- **Called with a parameter** â†’ builds `{env}_{param}` dynamically

```python
# src/core_utils/core/environment_config.py
import os
import json

# â”€â”€â”€ Update with YOUR workspace IDs â”€â”€â”€
WORKSPACE_ENV_MAP = {
    "1234567890123456": "dev",
    "2345678901234567": "staging",
    "3456789012345678": "prod",
}
DEFAULT_ENV = "dev"


class EnvironmentConfig:
    """
    Resolves Databricks workspace â†’ environment â†’ catalog/schema/paths.

    Examples:
        env = EnvironmentConfig()                          # auto-detect
        env = EnvironmentConfig(catalog="my_cat", schema="my_sch")  # explicit
    """

    def __init__(self, catalog: str = None, schema: str = None):
        self._workspace_id = self._detect_workspace_id()
        self._env = WORKSPACE_ENV_MAP.get(self._workspace_id, DEFAULT_ENV)
        self._default_catalog = catalog
        self._default_schema = schema

    # â”€â”€ Workspace Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @staticmethod
    def _detect_workspace_id() -> str:
        """Detect workspace ID via Spark conf â†’ notebook context â†’ env var."""
        # Method 1: Spark conf
        try:
            from pyspark.sql import SparkSession
            spark = SparkSession.builder.getOrCreate()
            ws_url = spark.conf.get("spark.databricks.workspaceUrl", "")
            if ws_url:
                # URL: adb-<workspace_id>.<random>.azuredatabricks.net
                parts = ws_url.split("-")
                if len(parts) > 1:
                    ws_id = parts[1].split(".")[0]
                    if ws_id.isdigit():
                        return ws_id
        except Exception:
            pass

        # Method 2: Notebook context (dbutils)
        try:
            ctx = json.loads(
                dbutils.notebook.entry_point       # noqa: F821
                .getDbutils().notebook()
                .getContext().toJson()
            )
            return str(ctx.get("tags", {}).get("orgId", ""))
        except Exception:
            pass

        # Method 3: Env var fallback
        return os.environ.get("DATABRICKS_WORKSPACE_ID", "unknown")

    @property
    def environment(self) -> str:
        """Current environment: 'dev', 'staging', or 'prod'."""
        return self._env

    @property
    def workspace_id(self) -> str:
        return self._workspace_id

    # â”€â”€ Catalog & Schema â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_catalog(self, name: str = None) -> str:
        """
        No arg â†’ returns default catalog.
        With arg â†’ builds "{env}_{name}".

            env.get_catalog()         â†’ "dev_catalog"   (or "my_cat" if initialized)
            env.get_catalog("sales")  â†’ "dev_sales"
        """
        if name is not None:
            return f"{self._env}_{name}"
        if self._default_catalog is not None:
            return self._default_catalog
        return f"{self._env}_catalog"

    def get_schema(self, name: str = None) -> str:
        """
        No arg â†’ returns default schema.
        With arg â†’ builds "{env}_{name}".

            env.get_schema()          â†’ "dev_bronze"   (or "my_sch" if initialized)
            env.get_schema("gold")    â†’ "dev_gold"
        """
        if name is not None:
            return f"{self._env}_{name}"
        if self._default_schema is not None:
            return self._default_schema
        return f"{self._env}_bronze"

    def get_full_table_name(self, table: str,
                            catalog: str = None,
                            schema: str = None) -> str:
        """Build catalog.schema.table."""
        return f"{self.get_catalog(catalog)}.{self.get_schema(schema)}.{table}"

    # â”€â”€ Volume & File Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_volumes_root(self, volume_name: str = "raw") -> str:
        """/Volumes/{catalog}/{schema}/{volume_name}"""
        return f"/Volumes/{self.get_catalog()}/{self.get_schema()}/{volume_name}"

    def get_source_path(self, relative_path: str,
                        volume_name: str = "raw") -> str:
        """/Volumes/{catalog}/{schema}/raw/{relative_path}"""
        return f"{self.get_volumes_root(volume_name)}/{relative_path}"

    # â”€â”€ Audit & Error Logs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_audit_log_table(self, table_name: str = None) -> str:
        """catalog.schema.audit_log (or custom name)."""
        return self.get_full_table_name(table_name or "audit_log")

    def get_error_log_table(self, table_name: str = None) -> str:
        """catalog.schema.error_log (or custom name)."""
        return self.get_full_table_name(table_name or "error_log")

    def get_error_log_path(self, subfolder: str = None) -> str:
        """/Volumes/.../logs/errors[/subfolder]"""
        path = f"{self.get_volumes_root('logs')}/errors"
        return f"{path}/{subfolder}" if subfolder else path

    def get_checkpoint_path(self, job_name: str) -> str:
        """/Volumes/.../checkpoints/{job_name}"""
        return f"{self.get_volumes_root('checkpoints')}/{job_name}"

    # â”€â”€ Utilities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def summary(self) -> dict:
        return {
            "workspace_id": self._workspace_id,
            "environment": self._env,
            "catalog": self.get_catalog(),
            "schema": self.get_schema(),
            "volumes_root": self.get_volumes_root(),
            "audit_log_table": self.get_audit_log_table(),
            "error_log_table": self.get_error_log_table(),
            "error_log_path": self.get_error_log_path(),
        }

    def __repr__(self) -> str:
        return (f"EnvironmentConfig(env='{self._env}', "
                f"catalog='{self.get_catalog()}', "
                f"schema='{self.get_schema()}')")
```

### Quick Reference Table

| Code | Result (dev) | Result (prod) |
|---|---|---|
| `EnvironmentConfig()` | env=dev | env=prod |
| `.get_catalog()` | `dev_catalog` | `prod_catalog` |
| `.get_catalog("sales")` | `dev_sales` | `prod_sales` |
| `.get_schema()` | `dev_bronze` | `prod_bronze` |
| `.get_schema("gold")` | `dev_gold` | `prod_gold` |
| `.get_full_table_name("txn")` | `dev_catalog.dev_bronze.txn` | `prod_catalog.prod_bronze.txn` |
| `.get_source_path("falcon/txn")` | `/Volumes/dev_catalog/dev_bronze/raw/falcon/txn` | `/Volumes/prod_catalog/prod_bronze/raw/falcon/txn` |
| `.get_audit_log_table()` | `dev_catalog.dev_bronze.audit_log` | `prod_catalog.prod_bronze.audit_log` |
| `.get_error_log_path("bronze")` | `/Volumes/.../logs/errors/bronze` | `/Volumes/.../logs/errors/bronze` |
| **With init overrides:** | | |
| `EnvironmentConfig(catalog="my_cat")` | | |
| `.get_catalog()` | `my_cat` | `my_cat` |
| `.get_catalog("sales")` | `dev_sales` | `prod_sales` |

> [!IMPORTANT]
> Update `WORKSPACE_ENV_MAP` with your actual workspace IDs before deploying.

---

## 4. `entrypoints.py`

```python
# src/core_utils/entrypoints.py
import sys


def bronze_entry():
    """
    Called via: run-bronze <config_path> <config_filename>
    Example:   run-bronze /Workspace/.../configs/bronze my_feed.yml
    """
    from pyspark.sql import SparkSession
    from core_utils.core.environment_config import EnvironmentConfig
    from core_utils.bronze.config_loader import load_bronze_config
    from core_utils.bronze.readers import read_source
    from core_utils.bronze.writer import write_to_target

    config_path = sys.argv[1]
    config_filename = sys.argv[2]
    spark = SparkSession.builder.getOrCreate()

    # 1. Detect environment
    env_config = EnvironmentConfig()
    print(f"ğŸŒ Environment: {env_config}")

    # 2. Load YAML config
    config = load_bronze_config(config_path, config_filename)

    # 3. Read source â†’ DataFrame
    df = read_source(spark, config, env_config)

    # 4. Write to Delta target
    write_to_target(spark, df, config["target"])
    print(f"âœ… Bronze complete: {config['target']['table']}")


def silver_entry():
    """Called via: run-silver <config_path> <config_filename>"""
    from pyspark.sql import SparkSession
    from core_utils.core.environment_config import EnvironmentConfig
    config_path = sys.argv[1]
    config_filename = sys.argv[2]
    spark = SparkSession.builder.getOrCreate()
    env_config = EnvironmentConfig()
    # TODO: Implement silver logic
    print(f"Silver entry â€” env={env_config.environment}")


def gold_entry():
    """Called via: run-gold <config_path> <config_filename>"""
    from pyspark.sql import SparkSession
    from core_utils.core.environment_config import EnvironmentConfig
    config_path = sys.argv[1]
    config_filename = sys.argv[2]
    spark = SparkSession.builder.getOrCreate()
    env_config = EnvironmentConfig()
    # TODO: Implement gold logic
    print(f"Gold entry â€” env={env_config.environment}")
```

---

## 5. `config_loader.py`

```python
# src/core_utils/bronze/config_loader.py
import os
import yaml

REQUIRED_SOURCE_FIELDS = ["format", "source_path"]
REQUIRED_TARGET_FIELDS = ["table", "mode"]
VALID_FORMATS = ["csv", "parquet", "txt", "json", "binaryfile"]
VALID_MODES = ["append", "overwrite", "merge"]


def load_bronze_config(config_path: str, config_filename: str) -> dict:
    """Load and validate a Bronze YAML config file."""
    full_path = os.path.join(config_path, config_filename)

    if not os.path.exists(full_path):
        raise FileNotFoundError(f"Config file not found: {full_path}")

    with open(full_path, "r") as f:
        config = yaml.safe_load(f)

    _validate_config(config, full_path)
    return config


def _validate_config(config: dict, file_path: str):
    if not config:
        raise ValueError(f"Empty config file: {file_path}")

    # Validate source
    if "source" not in config or "properties" not in config.get("source", {}):
        raise ValueError(f"Missing 'source.properties' in {file_path}")

    props = config["source"]["properties"]
    for field in REQUIRED_SOURCE_FIELDS:
        if field not in props:
            raise ValueError(f"Missing 'source.properties.{field}' in {file_path}")

    if props["format"] not in VALID_FORMATS:
        raise ValueError(f"Invalid format '{props['format']}'. Supported: {VALID_FORMATS}")

    # Validate target
    if "target" not in config:
        raise ValueError(f"Missing 'target' section in {file_path}")

    target = config["target"]
    for field in REQUIRED_TARGET_FIELDS:
        if field not in target:
            raise ValueError(f"Missing 'target.{field}' in {file_path}")

    if target["mode"] not in VALID_MODES:
        raise ValueError(f"Invalid mode '{target['mode']}'. Supported: {VALID_MODES}")

    if target["mode"] == "merge" and "merge_keys" not in target:
        raise ValueError(f"mode='merge' requires 'merge_keys' in {file_path}")
```

---

## 6. `readers.py` â€” `read_source` Dispatcher + Format Readers

```python
# src/core_utils/bronze/readers.py
from pyspark.sql import SparkSession, DataFrame


# â”€â”€ Format-specific readers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def read_csv(spark: SparkSession, path: str, options: dict) -> DataFrame:
    """Read CSV/delimited files. Options: header, inferSchema, delimiter, encoding, etc."""
    reader = spark.read.format("csv")
    for k, v in options.items():
        reader = reader.option(k, v)
    df = reader.load(path)
    print(f"  ğŸ“„ read_csv: {df.count()} rows from {path}")
    return df


def read_parquet(spark: SparkSession, path: str, options: dict) -> DataFrame:
    """Read Parquet files. Options: mergeSchema."""
    reader = spark.read.format("parquet")
    for k, v in options.items():
        reader = reader.option(k, v)
    df = reader.load(path)
    print(f"  ğŸ“„ read_parquet: {df.count()} rows from {path}")
    return df


def read_txt(spark: SparkSession, path: str, options: dict) -> DataFrame:
    """Read text/fixed-width files. Each line â†’ row with 'value' column."""
    reader = spark.read.format("text")
    for k, v in options.items():
        reader = reader.option(k, v)
    df = reader.load(path)
    print(f"  ğŸ“„ read_txt: {df.count()} rows from {path}")
    return df


def read_json(spark: SparkSession, path: str, options: dict) -> DataFrame:
    """Read JSON files. Options: multiLine, primitivesAsString."""
    reader = spark.read.format("json")
    for k, v in options.items():
        reader = reader.option(k, v)
    df = reader.load(path)
    print(f"  ğŸ“„ read_json: {df.count()} rows from {path}")
    return df


def read_binaryfile(spark: SparkSession, path: str, options: dict) -> DataFrame:
    """Read binary files (e.g., EBCDIC). Returns: path, modificationTime, length, content."""
    reader = spark.read.format("binaryFile")
    for k, v in options.items():
        reader = reader.option(k, v)
    df = reader.load(path)
    print(f"  ğŸ“„ read_binaryfile: {df.count()} files from {path}")
    return df


# â”€â”€ Format Registry â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# To add a new format: write the function + add one line here

FORMAT_READERS = {
    "csv":        read_csv,
    "parquet":    read_parquet,
    "txt":        read_txt,
    "json":       read_json,
    "binaryfile": read_binaryfile,
}


# â”€â”€ Main Dispatcher â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def read_source(spark: SparkSession, config: dict, env_config=None) -> DataFrame:
    """
    Dispatch to the right reader based on YAML config.
    Uses EnvironmentConfig for path resolution.
    """
    source_props = config["source"]["properties"]
    fmt = source_props["format"]
    source_path = source_props["source_path"]
    extension = source_props.get("extension", "")
    options = source_props.get("options", {})

    resolved_path = _resolve_source_path(source_path, extension, env_config)
    print(f"ğŸ” read_source: format={fmt}, path={resolved_path}")

    reader_fn = FORMAT_READERS.get(fmt)
    if reader_fn is None:
        raise ValueError(f"Unsupported format: '{fmt}'. Supported: {list(FORMAT_READERS.keys())}")

    return reader_fn(spark, resolved_path, options)


def _resolve_source_path(source_path: str, extension: str, env_config=None) -> str:
    """Resolve relative paths using EnvironmentConfig, pass absolute paths through."""
    if source_path.startswith("/") or source_path.startswith("abfss://"):
        resolved = source_path
    elif env_config is not None:
        resolved = env_config.get_source_path(source_path)
    else:
        resolved = f"/Volumes/raw/{source_path}"

    if extension and not resolved.endswith(extension):
        if not extension.startswith("*"):
            extension = f"*{extension}"
        resolved = f"{resolved}/{extension}"

    return resolved
```

---

## 7. `writer.py`

```python
# src/core_utils/bronze/writer.py
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import current_timestamp, lit


def write_to_target(spark: SparkSession, df: DataFrame, target_config: dict):
    """Write DataFrame to Delta table. Modes: append, overwrite, merge."""
    table = target_config["table"]
    mode = target_config["mode"]

    # Add Bronze metadata
    df = df.withColumn("__bronze_load_ts", current_timestamp())
    df = df.withColumn("__bronze_tag", lit("BRONZE_OK"))

    if mode in ("append", "overwrite"):
        df.write.format("delta").mode(mode).saveAsTable(table)
        print(f"  âœ… Written to {table} (mode={mode})")

    elif mode == "merge":
        merge_keys = target_config["merge_keys"]
        _delta_merge(spark, df, table, merge_keys)
        print(f"  âœ… Merged into {table} on keys={merge_keys}")

    else:
        raise ValueError(f"Unsupported write mode: {mode}")


def _delta_merge(spark: SparkSession, source_df: DataFrame,
                 target_table: str, merge_keys: list):
    """Delta MERGE (upsert). Creates table if it doesn't exist."""
    from delta.tables import DeltaTable

    if not spark.catalog.tableExists(target_table):
        source_df.write.format("delta").saveAsTable(target_table)
        return

    delta_table = DeltaTable.forName(spark, target_table)
    condition = " AND ".join([f"target.{k} = source.{k}" for k in merge_keys])

    delta_table.alias("target").merge(
        source_df.alias("source"), condition
    ).whenMatchedUpdateAll(
    ).whenNotMatchedInsertAll(
    ).execute()
```

---

## 8. YAML Config Examples

### CSV
```yaml
# configs/bronze/transactions_csv.yml
source:
  type: "file"
  properties:
    source_path: "falcon/transactions/credit"
    format: "csv"
    extension: ".csv"
    last_n_days: 7
    options:
      header: "true"
      inferSchema: "false"
      delimiter: ","
      encoding: "UTF-8"

target:
  table: "dev_catalog.falcon_bronze.credit_transactions_raw"
  mode: "append"
```

### Parquet
```yaml
# configs/bronze/events_parquet.yml
source:
  type: "file"
  properties:
    source_path: "falcon/events/clickstream"
    format: "parquet"
    extension: ".parquet"
    options:
      mergeSchema: "true"

target:
  table: "dev_catalog.falcon_bronze.clickstream_raw"
  mode: "overwrite"
```

### TXT / Fixed-Width
```yaml
# configs/bronze/legacy_txt.yml
source:
  type: "file"
  properties:
    source_path: "falcon/legacy/reports"
    format: "txt"
    extension: ".txt"
    options:
      lineSep: "\n"

target:
  table: "dev_catalog.falcon_bronze.legacy_reports_raw"
  mode: "append"
```

---

## 9. Databricks Job Configuration (DAB)

### Single Bronze Job
```yaml
# resources/bronze_job.yml
resources:
  jobs:
    bronze_loader:
      name: "bronze-loader-${bundle.target}"

      parameters:
        - name: config_path
          default: "/Workspace/.bundle/${bundle.name}/${bundle.target}/configs/bronze"
        - name: config_filename
          default: ""

      tasks:
        - task_key: bronze_load
          python_wheel_task:
            package_name: core_utils
            entry_point: run-bronze
            parameters:
              - "{{job.parameters.config_path}}"
              - "{{job.parameters.config_filename}}"
          libraries:
            - whl: /Volumes/dev_catalog/default/wheels/core_utils-1.0.0-py3-none-any.whl
          job_cluster_key: bronze_cluster

      job_clusters:
        - job_cluster_key: bronze_cluster
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            num_workers: 2
            node_type_id: "Standard_DS3_v2"
```

### Multi-Task Pipeline (Bronze â†’ Silver â†’ Gold)
```yaml
# resources/etl_pipeline_job.yml
resources:
  jobs:
    etl_pipeline:
      name: "etl-pipeline-${bundle.target}"

      parameters:
        - name: config_path
          default: "/Workspace/.bundle/${bundle.name}/${bundle.target}/configs"
        - name: bronze_config
          default: ""
        - name: silver_config
          default: ""
        - name: gold_config
          default: ""

      tasks:
        - task_key: bronze_load
          python_wheel_task:
            package_name: core_utils
            entry_point: run-bronze
            parameters:
              - "{{job.parameters.config_path}}/bronze"
              - "{{job.parameters.bronze_config}}"
          libraries:
            - whl: /Volumes/dev_catalog/default/wheels/core_utils-1.0.0-py3-none-any.whl
          job_cluster_key: etl_cluster

        - task_key: silver_transform
          depends_on: [{task_key: bronze_load}]
          python_wheel_task:
            package_name: core_utils
            entry_point: run-silver
            parameters:
              - "{{job.parameters.config_path}}/silver"
              - "{{job.parameters.silver_config}}"
          libraries:
            - whl: /Volumes/dev_catalog/default/wheels/core_utils-1.0.0-py3-none-any.whl
          job_cluster_key: etl_cluster

        - task_key: gold_aggregate
          depends_on: [{task_key: silver_transform}]
          python_wheel_task:
            package_name: core_utils
            entry_point: run-gold
            parameters:
              - "{{job.parameters.config_path}}/gold"
              - "{{job.parameters.gold_config}}"
          libraries:
            - whl: /Volumes/dev_catalog/default/wheels/core_utils-1.0.0-py3-none-any.whl
          job_cluster_key: etl_cluster

      job_clusters:
        - job_cluster_key: etl_cluster
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            num_workers: 4
            node_type_id: "Standard_DS4_v2"
```

---

## 10. End-to-End Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Databricks Job Trigger                                         â”‚
â”‚  params: config_path="/Workspace/.../configs/bronze"            â”‚
â”‚          config_filename="transactions_csv.yml"                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  bronze_entry()                                                  â”‚
â”‚                                                                  â”‚
â”‚  1. EnvironmentConfig() â†’ workspace_id â†’ env="dev"              â”‚
â”‚  2. load_bronze_config(path, filename) â†’ validates YAML         â”‚
â”‚  3. read_source(spark, config, env_config) â†’ dispatches reader  â”‚
â”‚  4. write_to_target(spark, df, target_config) â†’ Delta table     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EnvironmentConfig                                               â”‚
â”‚  workspace_id â†’ "1234567890123456" â†’ env = "dev"                â”‚
â”‚  get_catalog()         â†’ "dev_catalog"                          â”‚
â”‚  get_source_path(...)  â†’ "/Volumes/dev_catalog/dev_bronze/raw/" â”‚
â”‚  get_audit_log_table() â†’ "dev_catalog.dev_bronze.audit_log"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  read_source(spark, config, env_config)                          â”‚
â”‚  format="csv" â†’ FORMAT_READERS["csv"] â†’ read_csv()              â”‚
â”‚  path resolved via env_config.get_source_path()                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼           â–¼           â–¼           â–¼
       read_csv()  read_parquet() read_txt()  read_json() ...
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  write_to_target()                                               â”‚
â”‚  + __bronze_load_ts, __bronze_tag columns                       â”‚
â”‚  â†’ dev_catalog.falcon_bronze.credit_transactions_raw            â”‚
â”‚  mode: append / overwrite / merge                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 11. Build & Deploy

```bash
# Build the wheel
pip install build
python -m build --wheel --outdir dist/
# Output: dist/core_utils-1.0.0-py3-none-any.whl

# Upload to Databricks Volumes
databricks fs cp dist/core_utils-1.0.0-py3-none-any.whl \
  dbfs:/Volumes/dev_catalog/default/wheels/

# OR publish to Azure Artifacts (for CI/CD)
twine upload --repository-url https://pkgs.dev.azure.com/... dist/*.whl
```

---

## 12. Verification Plan

| Step | What to Verify |
|---|---|
| 1. Build | `python -m build --wheel` succeeds |
| 2. Install | `pip install dist/core_utils-1.0.0-py3-none-any.whl` installs cleanly |
| 3. Entry points exist | `run-bronze`, `run-silver`, `run-gold` are available after install |
| 4. Config validation | `load_bronze_config()` correctly validates/rejects YAML configs |
| 5. EnvironmentConfig | Verify workspace detection + catalog/schema building on Databricks |
| 6. E2E on Databricks | Upload wheel â†’ create job â†’ trigger with CSV config â†’ verify Delta table |
